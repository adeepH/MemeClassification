{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embracenet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4yYV9LOWDiH",
        "outputId": "5efc3111-6192-45ba-c731-e21200d0839f"
      },
      "source": [
        "!pip install transformers==3.3.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 11.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/26/c02ba92ecb8b780bdae4a862d351433c2912fe49469dac7f87a5c85ccca6/tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 35.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 36.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 37.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.95 tokenizers-0.8.1rc2 transformers-3.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsZlB7jVWTHd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "import time\n",
        "from shutil import copy2\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW,get_linear_schedule_with_warmup,AutoModel,AutoTokenizer\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAJndIYiWVw5"
      },
      "source": [
        "extract_path = '/content/drive/MyDrive/Colab Notebooks/Datasets and weights/Tamil_troll_memes/training_img.zip'\n",
        "with ZipFile(extract_path, 'r') as zipObj:\n",
        "   zipObj.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE771Ut1elzc"
      },
      "source": [
        "def move_data(start,troll,not_troll):\n",
        "  for img_name in os.listdir(start):\n",
        "    src = os.path.join(start,img_name)\n",
        "    if img_name.startswith('N'):\n",
        "      copy2(src,not_troll)\n",
        "    else:\n",
        "      copy2(src,troll)\n",
        "\n",
        "os.mkdir('Troll')\n",
        "os.mkdir('Non_troll')\n",
        "src = '/content/uploaded_tamil_memes'\n",
        "move_data(src,'/content/Troll','/content/Non_troll')\n",
        "\n",
        "def split_data(start,train,val,split):\n",
        "  for i, img_name in enumerate(os.listdir(start)):\n",
        "    src = os.path.join(start,img_name)\n",
        "    if i < split:\n",
        "      copy2(src,val)\n",
        "    else:\n",
        "      copy2(src,train)\n",
        "\n",
        "os.mkdir('Train')\n",
        "os.mkdir('Val')\n",
        "split_data('/content/Troll','/content/Train','/content/Val',128)\n",
        "split_data('/content/Non_troll','/content/Train','/content/Val',101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry75v5BNWi0k"
      },
      "source": [
        "class TamilDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,df,tokenizer,max_len,path,transforms=None):\n",
        "    self.data_dir = path\n",
        "    self.df = df\n",
        "    self.tokenizer = tokenizer\n",
        "    self.transforms = transforms\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.df.shape[0]\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    img_name, captions = self.df.iloc[index]\n",
        "    img_path = os.path.join(self.data_dir,img_name)\n",
        "    labels = 0 if img_name.startswith('N') else 1\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    if self.transforms is not None:\n",
        "      img = self.transforms(img)\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "        captions,\n",
        "        add_special_tokens=True,\n",
        "        max_length = self.max_len,\n",
        "        return_token_type_ids = False,\n",
        "        padding = 'max_length',\n",
        "        return_attention_mask= True,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'image' : img,\n",
        "        'text' : captions,\n",
        "        'input_ids' : encoding['input_ids'].flatten(),\n",
        "        'attention_mask' : encoding['attention_mask'].flatten(),\n",
        "        'label' : torch.tensor(labels,dtype=torch.float)\n",
        "    } "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b54t0GIzWmmK"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets and weights/Tamil_troll_memes/train_captions.csv')\n",
        "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "\n",
        "train_df_data = []\n",
        "val_df_data = []\n",
        "for img_name in os.listdir('/content/Train'):\n",
        "  ind = list(df[df['imagename'] == img_name].index)[0]\n",
        "  train_df_data.append([img_name,df['captions'].iloc[ind]])\n",
        "\n",
        "for img_name in os.listdir('/content/Val'):\n",
        "  ind = list(df[df['imagename'] == img_name].index)[0]\n",
        "  val_df_data.append([img_name,df['captions'].iloc[ind]])\n",
        "\n",
        "train_df = pd.DataFrame(train_df_data,columns=['img_name','captions'])\n",
        "val_df = pd.DataFrame(val_df_data,columns=['img_name','captions'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maRMElhHWqxn"
      },
      "source": [
        "def create_data_loader(df,tokenizer,max_len,batch_size,mytransforms,path,shuffle):\n",
        "  ds = TamilDataset(\n",
        "      df,\n",
        "      tokenizer,\n",
        "      max_len,\n",
        "      path,\n",
        "      mytransforms\n",
        "  )\n",
        "\n",
        "  return DataLoader(ds,\n",
        "                    batch_size = batch_size,\n",
        "                    shuffle=False,\n",
        "                    num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJCPh83iWtjG"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "PRE_TRAINED_MODEL_NAME = 'distilbert-base-multilingual-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E-_vpgqWvKW"
      },
      "source": [
        "my_trans = transforms.Compose([\n",
        "        transforms.Resize((300,300)),\n",
        "        transforms.ToTensor(),     \n",
        "])\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 128\n",
        "train_data_loader = create_data_loader(train_df,tokenizer,MAX_LEN,BATCH_SIZE,my_trans,'/content/Train',True)\n",
        "val_data_loader = create_data_loader(val_df,tokenizer,MAX_LEN,BATCH_SIZE,my_trans,'/content/Val',False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJyVhtVAWy6Z"
      },
      "source": [
        "class Inception(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Inception,self).__init__()\n",
        "    self.inception = models.inception_v3(True)\n",
        "    self.Conv2d_1a_3x3 = self.inception.Conv2d_1a_3x3\n",
        "    self.Conv2d_2a_3x3 = self.inception.Conv2d_2a_3x3\n",
        "    self.Conv2d_2b_3x3 = self.inception.Conv2d_2b_3x3\n",
        "    self.maxpool1 = self.inception.maxpool1\n",
        "    self.Conv2d_3b_1x1 = self.inception.Conv2d_3b_1x1\n",
        "    self.Conv2d_4a_3x3 = self.inception.Conv2d_4a_3x3\n",
        "    self.maxpool2 = self.inception.maxpool2\n",
        "    self.Mixed_5b = self.inception.Mixed_5b\n",
        "    self.Mixed_5c = self.inception.Mixed_5c\n",
        "    self.Mixed_5d = self.inception.Mixed_5d\n",
        "    self.Mixed_6a = self.inception.Mixed_6a\n",
        "    self.Mixed_6b = self.inception.Mixed_6b\n",
        "    self.Mixed_6c = self.inception.Mixed_6c\n",
        "    self.Mixed_6d = self.inception.Mixed_6d\n",
        "    self.conv1 = nn.Conv2d(768,1280,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.Conv2d_1a_3x3(x)\n",
        "    x = self.Conv2d_2a_3x3(x)\n",
        "    x = self.Conv2d_2b_3x3(x)\n",
        "    x = self.maxpool1(x)\n",
        "    x = self.Conv2d_3b_1x1(x)\n",
        "    x = self.Conv2d_4a_3x3(x)\n",
        "    x = self.maxpool2(x)\n",
        "    x = self.Mixed_5b(x)\n",
        "    x = self.Mixed_5c(x)\n",
        "    x = self.Mixed_5d(x)\n",
        "    x = self.Mixed_6a(x)\n",
        "    x = self.Mixed_6b(x)\n",
        "    x = self.Mixed_6c(x)\n",
        "    x = self.Mixed_6d(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu7oc8IVW08O"
      },
      "source": [
        "class multimodal(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(multimodal, self).__init__()\n",
        "    self.auto = AutoModel.from_pretrained('distilbert-base-multilingual-cased')\n",
        "    self.inception = Inception()\n",
        "    self.mha = nn.MultiheadAttention(768,16,0.1)\n",
        "    self.fc1 = nn.Linear(98304,2048)\n",
        "    self.fc2 = nn.Linear(2048,512)\n",
        "    self.drop = nn.Dropout(0.3)\n",
        "    self.fc3 = nn.Linear(512,1)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "  def forward(self,input_ids, attention_mask,img):\n",
        "    output1 = self.auto(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    f_half = img[:,:,:,:150]\n",
        "    s_half = img[:,:,150:]\n",
        "    output2 = self.inception(f_half)\n",
        "    output3 = self.inception(s_half)\n",
        "\n",
        "    output1 = output1[0].permute(1,0,2)\n",
        "    batch_size, channels, width, height = output2.shape\n",
        "    output2 = output2.view(batch_size,channels,width * height).permute(2,0,1)\n",
        "    output3 = output3.view(batch_size,channels,width * height).permute(2,0,1)\n",
        "\n",
        "    out = self.mha(output1,output2,output3)[0]\n",
        "    len,batch_size,embed = out.shape\n",
        "    out = out.permute(1,0,2)\n",
        "    out = out.reshape(batch_size,len * embed)\n",
        "\n",
        "    out = self.fc1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.drop(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.drop(out)\n",
        "    out = self.fc3(out)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSbzbbiKW25X"
      },
      "source": [
        "model = multimodal()\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4USbv72W5Kw"
      },
      "source": [
        "EPOCHS = 5\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader)  * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "def epoch_time(start_time,end_time):\n",
        "\telapsed_time = end_time - start_time\n",
        "\telapsed_mins = int(elapsed_time/60)\n",
        "\telapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "\treturn elapsed_mins,elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQrmJHh3XLm6"
      },
      "source": [
        "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for idx, data in enumerate(data_loader):\n",
        "\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_mask'].to(device)\n",
        "        labels = data['label'].to(device)\n",
        "        labelsviewed = labels.view(labels.shape[0],1)\n",
        "        image = data['image'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            img=image\n",
        "            )\n",
        "        sig_outs = torch.sigmoid(outputs)\n",
        "        preds = [0 if x < 0.5 else 1 for x in outputs]\n",
        "        preds = torch.tensor(preds).to(device)\n",
        "        loss = loss_fn(outputs,labelsviewed)\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      labels = d[\"label\"].to(device)\n",
        "      labelsviewed = labels.view(labels.shape[0],1)\n",
        "      image = d['image'].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        img=image\n",
        "      )\n",
        "      sig_outs = torch.sigmoid(outputs)\n",
        "      preds = [0 if x < 0.5 else 1 for x in outputs]\n",
        "      preds = torch.tensor(preds).to(device)\n",
        "      loss = loss_fn(outputs, labelsviewed)\n",
        "      correct_predictions += torch.sum(preds == labels)\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOuBDCuVXRco",
        "outputId": "a709219d-31d7-4546-89c8-bccf7809b7dd"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "for epoch in range(EPOCHS):\n",
        " \n",
        " \n",
        "  start_time = time.time()\n",
        "  train_acc,train_loss = train_epoch(\n",
        "      model,\n",
        "      train_data_loader,\n",
        "      loss,\n",
        "      optimizer,\n",
        "      device,\n",
        "      scheduler,\n",
        "      2071\n",
        "  )\n",
        "   \n",
        "  \n",
        "  val_acc,val_loss = eval_model(\n",
        "      model,\n",
        "      val_data_loader,\n",
        "      loss,\n",
        "      device,\n",
        "      229\n",
        "  )\n",
        "  \n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "  print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'Train Loss {train_loss} accuracy {train_acc}')\n",
        "  print(f'Val Loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 2m 30s\n",
            "Train Loss 0.46982091149458516 accuracy 0.8387252535007242\n",
            "Val Loss 0.4254985024531682 accuracy 0.9388646288209607\n",
            "\n",
            "Epoch: 02 | Epoch Time: 2m 29s\n",
            "Train Loss 0.3613141480546731 accuracy 0.9623370352486721\n",
            "Val Loss 0.3639034350713094 accuracy 0.9694323144104803\n",
            "\n",
            "Epoch: 03 | Epoch Time: 2m 29s\n",
            "Train Loss 0.3383737903661453 accuracy 0.9797199420569773\n",
            "Val Loss 0.38473743895689644 accuracy 0.9475982532751092\n",
            "\n",
            "Epoch: 04 | Epoch Time: 2m 28s\n",
            "Train Loss 0.3165285150019022 accuracy 0.987445678416224\n",
            "Val Loss 0.4382855733235677 accuracy 0.9432314410480349\n",
            "\n",
            "Epoch: 05 | Epoch Time: 2m 28s\n",
            "Train Loss 0.3139743469368953 accuracy 0.9898599710284887\n",
            "Val Loss 0.44933277666568755 accuracy 0.9432314410480349\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0vrblR3XiHA"
      },
      "source": [
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets and weights/Tamil_troll_memes/test_captions.csv')\n",
        "df_test.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "extract_path = '/content/drive/MyDrive/Colab Notebooks/Datasets and weights/Tamil_troll_memes/test_img.zip'\n",
        "with ZipFile(extract_path, 'r') as zipObj:\n",
        "   zipObj.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V07vR8RzczTs"
      },
      "source": [
        "test_data_loader = create_data_loader(df_test,tokenizer,MAX_LEN,BATCH_SIZE,my_trans,'/content/test_img',False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqvM-dp9c3xS"
      },
      "source": [
        "def get_predictions(model,data_loader, device):\n",
        "    model = model.eval()\n",
        "    f_preds = []\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            image = d['image'].to(device)\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                img=image\n",
        "            )\n",
        "            sig_outs = torch.sigmoid(outputs)\n",
        "            preds = [0 if x < 0.5 else 1 for x in sig_outs]\n",
        "            for j in preds:\n",
        "                f_preds.append(j)\n",
        "    \n",
        "    return f_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJsmj7K8c59s"
      },
      "source": [
        "submission_preds = get_predictions(model,test_data_loader,device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZI3Bd_1c7_a"
      },
      "source": [
        "df_org = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets and weights/Tamil_troll_memes/gold_labels_for_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAKD9li1dC-u"
      },
      "source": [
        "def f(x):\n",
        "    if x == 'troll':\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "df_org['label'] = df_org['label'].apply(lambda x : f(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGWgjNJUdElN"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVRrfSJcdHAJ"
      },
      "source": [
        "print(classification_report(df_org['label'],submission_preds,target_names=['Non-Troll','Troll']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lKJMVtCdJAb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}